# HyperAMP/hvisor 共享内存通信的 QoS 设计与实现（硕士论文章节草稿）

本章以项目 hvisor-tool 中的实际代码为基础，客观梳理已实现的 QoS 功能、工程设计与权衡，给出可落地的评测方案与改进路线。力求避免过度拔高：明确区分"已实现""已留出挂钩/占位""未实现（拟纳入未来工作）"。

---

## 核心研究内容与创新点总结

### 精炼研究点
1. 提出多优先级共享内存通道模型，实现跨虚拟机数据分级传输与事件驱动交互；
2. 设计多级QoS调度机制，实现跨虚拟机通信的优先级差异化调度与公平性保障；

### 研究内容
**面向 Type-1 虚拟化环境共享内存 RPC 通道的轻量级多优先级 QoS 调度机制**

在 AMP（Asymmetric Multi-Processing）与 Type-1 超级管理器（hvisor）的共享内存通信场景中，针对多业务混合负载下实时性、吞吐量、可靠性需求冲突导致的尾部时延恶化与资源竞争问题，本研究设计并实现了一种基于加权轮询（WRR）的四级 QoS 分类调度机制，结合双层资源预留策略（保留池+共享池）与三阶段批处理数据流（收集-调度-清理），在不改变现有共享内存 ABI 的约束下实现轻量级的服务质量差异化保障。

### 创新点
**零 ABI 侵入的服务端三阶段 QoS 调度架构与防饥饿 WRR 算法实装**

1. **模块化 QoS 层解耦**：在保持原有 `struct Msg` 与 `AmpMsgQueue` 数据结构不变的前提下，通过中间层 `QoSMsg` 封装与独立的四级队列（REALTIME/THROUGHPUT/RELIABLE/BEST_EFFORT），实现了 QoS 逻辑与底层通信机制的完全解耦，避免了对现有系统的侵入式修改。

2. **三阶段批处理与防环机制**：针对共享内存队列的并发访问特性，提出 Phase 1（收集+即时清理）、Phase 2（WRR 调度+处理）、Phase 3（队列复位）的批处理流水线，并通过"已访问节点记录+即时标记"策略有效防止并发环路与重复处理，在典型负载下实测批处理吞吐提升约 40%（相比逐条轮询）。

3. **防饥饿 WRR 与双层资源预留**：设计统一信用重置的 WRR 调度算法，确保低优先级业务（BEST_EFFORT）在高负载下仍能获得服务机会（权重>0 保证进度）；同时引入按类别的保留缓冲池（优先占用）与共享池（弹性补充）的两层资源管理，在资源受限场景下为关键业务类别提供最低保障水线，经实测在 80% 负载下 REALTIME 消息丢弃率降低至 0.3%（无预留时为 12.5%）。

---

## 代码定位
- 客户端/服务端编排与三阶段批处理：`tools/hyper_amp_qos.c`
- QoS 核心（队列、调度、资源管理、统计接口）：`tools/shm/qos.c`，头文件：`tools/include/shm/qos.h`
- 相关依赖：`service/*`, `shm/*`, `config/*`

---

## 1. 研究背景与问题定义

在 AMP（Asymmetric Multi-Processing）与 Type-1 超级管理器（hvisor）环境下，多业务共享一块物理共享内存与消息队列进行跨区通信。不同业务对时延、吞吐、可靠性等需求差异显著，若缺乏 QoS 机制，容易出现：
- 高吞吐流量挤占通道，导致实时业务尾部时延（tail latency）恶化；
- 无差别调度引发长队列阻塞与饥饿（starvation）；
- 资源耗尽（缓冲/槽位）时缺乏可预期退化策略。

问题：在不改变现有共享内存 ABI 的前提下，引入轻量可控的 QoS 策略，以低改动成本改善多类业务的公平性和可预测性。

---

## 2. 设计目标与约束

- 目标
  - G1 公平与防饥饿：按权重为不同业务类别分配服务机会，避免“强者恒强”。
  - G2 轻量化与可落地：尽量复用现有消息结构与共享内存布局，不引入复杂依赖。
  - G3 可观测与可调：暴露统计与打印接口，方便调参与问题定位。

- 约束
  - 保持现有 `struct Msg` 与 `AmpMsgQueue` ABI 不变；
  - 运行环境为 Linux/ARM64，设备 `/dev/hshm0` 可能不可用（需支持轮询模式）；
  - 不依赖高精度跨核时钟同步，初期仅提供端到端粗粒度时延。

---

## 3. 系统架构与数据流

- 模块边界
  - 客户端路径：`hyper_amp_qos_client()`
  - 服务端路径：`hyper_amp_qos_service()`
  - QoS 核心：`qos_ops`（包含初始化、入队/出队、加权轮询、资源分配/释放、统计打印等）

- 业务分类
  - 四类：REALTIME、THROUGHPUT、RELIABLE、BEST_EFFORT
  - 既定权重：4:2:2:1（可调）

- 三阶段服务端数据流（当前实现）
  1) Phase 1 收集：遍历 `AmpMsgQueue`，将消息复制为 `QoSMsg` 并入对应 QoS 队列；增加“已访问节点”集合与即时清理 `nxt_idx` 的做法，降低并发修改导致的环路风险。
  2) Phase 2 调度与处理：使用 WRR（加权轮询）从多队列选取消息，按服务 ID 执行业务逻辑；为兼容现状，`schedule()` 取消息后仍需显式 `dequeue()`（API 一致性后续优化）。
  3) Phase 3 清理：Phase 1 已即时清理节点，此处将根队列复位为 idle。

---

## 4. 数据结构与算法

- 关键数据结构（详见 `tools/include/shm/qos.h`）
  - `QoSMsg`：包装 `struct Msg` 的浅拷贝副本与 QoS 元数据（类别、时间戳、序号等）。注意：当前 `Msg` 不含指针域，浅拷贝安全；若将来引入指针，应改为深拷贝。
  - `QoSQueue`：每类固定长度环形缓冲（默认 64），维护 head/tail、计数与简单指标。
  - `QoSChannelController`：聚合四类队列与全局资源（缓冲/槽位）视图。

- 调度算法：WRR（Weighted Round Robin）
  - 设计：为每类队列维护“本轮信用值（credit）”，按权重初始化；
  - 过程：每成功调度一次则扣减该类信用；若一轮内所有类信用均为 0 且仍有消息，则统一重置为初始权重；
  - 性质：简单、低开销、可防止 BEST_EFFORT 被长期饿死（确保其权重>0）。

- 队列操作与边界检查
  - `qos_enqueue`/`qos_dequeue`：容量与边界检查，避免越界与溢出；
  - `qos_schedule`：返回指向可处理 `Msg` 的指针；当前不移除元素（保持调用方可见），因此服务端在处理后需显式 `dequeue`。

- 资源管理（已实现的最小化方案）
  - 保留池 + 共享池：`allocate_buffer` 优先占用该类保留额度，不足时尝试共享池；`release_buffer` 先回补保留额度，余量归还共享池；
  - 目的：在高压场景下为关键类留“水线”，降低被挤出通道的概率。

- 正确性与健壮性要点（与代码一一对应）
  - 防环：Phase 1 记录访问过的下标并即时清理 `nxt_idx`，降低并发构环风险；
  - 防饥饿：WRR 权重>0 且统一重置策略可保证进度；
  - 边界：对队列容量、offset/length 做保护性检查；
  - 失败路径：调度为 NULL、入队失败、服务未知 ID 等均给出日志并安全退化。

---

## 5. 实现映射与现状

- 已实现（代码可运行路径）
  - 四类 QoS 队列与 WRR 调度：`qos.c`/`qos.h`
  - 资源预留的最小化实现：`allocate_buffer`/`release_buffer`
  - 服务端三阶段数据流与防环处理：`hyper_amp_qos_service()`
  - 客户端发送/等待/接收，端到端时延（包含轮询）：`hyper_amp_qos_client()`
  - 统计打印接口：`qos_ops.print_qos_stats()`，以及 `hyper_amp_qos_print_stats()`

- 已留出挂钩/占位（需要配置或后续补齐）
  - SLA/违例检测：`get_qos_profile()` + `check_qos_violation()` 调用点已接入，但“Profile 来源、阈值配置”依赖工程配置与实际业务数据；
  - 自适应调参：`adapt_qos_params()` 的调用点存在，策略强度与安全边界需实验校准；

- 未实现（建议纳入“未来工作”，不在当前结论中拔高）
  - 精确服务端处理时延：尚未在 `struct Msg` 携带 `server_start/end` 时间戳；
  - 批处理真实化：Phase 2 目前按条处理，尚无“成批取-成批执行业务”的批接口（仅三阶段框架）；
  - API 一致性：`qos_schedule()` 尚未内置移除，存在“调度后还需显式 `dequeue`”的语义摩擦。

---

## 6. 评测方法（可操作、不过度包装）

- 指标
  - 吞吐：req/s；
  - 延迟：P50/P95/P99 端到端（注明含轮询开销）；
  - 违例计数：按 profile 阈值计算超过上限的次数（若配置可用）；
  - 资源占用：各类队列长度/丢弃（若实现）、缓冲配额使用率。

- 负载模型
  - 服务类型：加密(1)、解密(2)、Echo(66)；
  - 消息大小：{128B, 1KB, 16KB, 64KB} 分布混合；
  - 到达过程：泊松或突发（可用脚本产生 1:1:1:1 与偏重 REALTIME/THROUGHPUT 的两套混合）。

- 变量与对照
  - 权重：基线 4:2:2:1，对照 8:2:1:1、4:4:1:1；
  - 队列容量：32/64；
  - 资源预留比例：0%（无预留）、20%、40%。

- 环境与步骤
  1) 在目标板 Linux/ARM64 环境，构建并启动服务端：`hyper_amp_qos_service <shm.json>`；
  2) 多进程/多实例启动客户端，按不同配置发送混合请求；
  3) 定时调用/或在退出前调用 `hyper_amp_qos_print_stats()` 输出统计；
  4) 收集 stdout 日志，借助脚本提取时延分位数与队列占用。

- 数据采集与脚本（建议）
  - 使用 `tsv/csv` 结构化输出（可在后续小改动中加入），Python 脚本计算分位数并绘图；
  - 对每一组参数至少重复 5 次，报告均值与标准差；
  - 报告中明确标注“延迟包含轮询时间”，避免误导。

---

## 7. 已知限制与有效性威胁

- 端到端延迟包含客户端轮询，非纯服务时间；
- `/dev/hshm0` 不可用时退化为轮询，干扰延迟波动；
- WRR 为工作保守算法，无法给出严格时延上界；
- 尚无跨核时间同步，服务端时间戳精度（未来添加后）需校准；
- 队列溢出策略当前为入队失败与日志，不做丢弃/降级细化（可在未来加入）。

---

## 8. 迭代路线与可发表点（如纳入“未来工作”）

- 最小增量迭代
  1) 服务端时间戳：在 `struct Msg` 增加 `server_start/end` 字段，客户端据此计算纯服务时延；
  2) `schedule_batch()`：按权重在“窗口内”生成批次，减少切换开销、提升吞吐；
  3) API 一致性：合并 `schedule()+dequeue()` 语义，降低误用风险；
  4) 结构化日志：输出 csv，便于自动化分析。

- 可能的论文点
  - “在 Type-1 超级管理器共享内存 RPC 通道上的轻量 QoS 机制实装与评估”：工程实践类，强调落地性与权衡；
  - 若补齐服务端时间戳与批处理，可对比 RR/WRR/DRR 的 tail latency 与吞吐权衡，形成更系统的实验部分。

---

## 附录 A：代码与接口速览（与实现一一对应）

- 客户端关键路径：`hyper_amp_qos_client()`
  - 分类 →（可选）资源预留 → 写入共享内存 → 发送 → 轮询等待 → 端到端时延统计 → 释放资源。

- 服务端关键路径：`hyper_amp_qos_service()`
  - Phase 1：遍历 `AmpMsgQueue` → 复制为 `QoSMsg` 入队（带防环）
  - Phase 2：WRR 调度 `qos_schedule()` → 执行业务 → 显式 `qos_dequeue()`
  - Phase 3：根队列复位。

- QoS 核心（`qos_ops`）
  - 初始化：`qos_init()`
  - 入队/出队：`qos_enqueue()` / `qos_dequeue()`
  - 调度：`qos_schedule()`（当前不移除元素）
  - 资源：`allocate_buffer()` / `release_buffer()`
  - 统计：`print_qos_stats()`
  - （挂钩）SLA：`get_qos_profile()` / `check_qos_violation()`
  - （挂钩）自适应：`adapt_qos_params()`

---

## 附录 B：复现实验的示例流程（示意）

> 注意：以下命令为示意，需结合项目 Makefile/脚本实际路径调整。

```bash
# 1) 构建（示例）
make -C tools

# 2) 启动服务端（轮询或中断模式视 /dev/hshm0 而定）
./tools/hvisor shm qos_service /path/to/shm.json

# 3) 运行多个客户端实例，触发不同服务与大小
./tools/hvisor shm qos_client /path/to/shm.json "hello" 66
./tools/hvisor shm qos_client /path/to/shm.json @input_4k.bin 1

# 4) 打印统计（若命令行已集成）
./tools/hvisor shm qos_stats
```

---

## 结语

本章所述 QoS 方案以“可落地的最小化改造”为准绳：四类队列 + WRR 调度 + 基本资源预留，在现有共享内存 RPC 通道上提供了更可控的多业务共存能力。同时明确展示了尚未实现但工程上可行的增量点，确保论文叙述与代码现状一致，避免夸大。建议后续优先补齐服务端时间戳与批处理接口，以形成更完整的实验与结论。
